#!/usr/bin/env python3
"""
Script d'entra√Ænement pour cr√©er un mod√®le RL g√©n√©ralis√©.

Entra√Æne un mod√®le unique qui peut √™tre utilis√© par tous les utilisateurs
gr√¢ce √† la normalisation des √©tats.
"""

import sys
import os
import random
import logging
import time
from pathlib import Path
from typing import List

# Ajouter le r√©pertoire backend au path
sys.path.insert(0, str(Path(__file__).parent))

from app.services.monte_carlo.rl.config import RLConfig
from app.services.monte_carlo.rl.trainer import RLTrainer, optimize_rl_config_for_m4_pro
# RetirementEnvironment n'est plus n√©cessaire ici, c'est RLTrainer qui le g√®re
from app.schemas.projections import (
    SavingsOptimizationInput,
    AdultProfile,
    InvestmentAccount,
    MarketAssumptions,
    SavingsPhase,
    SpendingPhase,
    InvestmentAccountType,
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def generate_varied_profiles(num_profiles: int = 50) -> List[SavingsOptimizationInput]:
    """
    G√©n√®re une vari√©t√© de profils pour l'entra√Ænement.
    
    Args:
        num_profiles: Nombre de profils √† g√©n√©rer
    
    Returns:
        Liste de profils d'optimisation vari√©s
    """
    profiles = []
    
    # Varier les param√®tres
    ages = list(range(25, 61, 5))  # 25, 30, 35, ..., 60
    retirement_ages = [60, 62, 65, 67, 70]
    initial_capitals = [0, 10000, 50000, 100000, 200000, 500000]
    monthly_incomes = [2000, 3000, 4000, 5000, 6000, 8000]
    target_incomes = [1500, 2000, 2500, 3000, 4000]
    
    # Hypoth√®ses de march√© standard
    market_assumptions = MarketAssumptions(
        inflation_mean=2.0,
        inflation_volatility=1.0,
        asset_classes={
            "equities": {
                "label": "Actions mondiales",
                "expected_return": 7.0,
                "volatility": 15.0,
            },
            "bonds": {
                "label": "Obligations",
                "expected_return": 3.0,
                "volatility": 6.0,
            },
        },
        correlations={
            "equities": {"equities": 1.0, "bonds": 0.3},
            "bonds": {"equities": 0.3, "bonds": 1.0},
        },
    )
    
    for i in range(num_profiles):
        # S√©lectionner des param√®tres vari√©s
        age = random.choice(ages)
        # S'assurer qu'il y a un retirement_age valide
        valid_retirement_ages = [a for a in retirement_ages if a > age + 10]
        if not valid_retirement_ages:
            # Si aucun retirement_age valide, utiliser l'√¢ge + 15 comme minimum
            retirement_age = age + 15
        else:
            retirement_age = random.choice(valid_retirement_ages)
        life_expectancy = retirement_age + random.choice([15, 20, 25])
        initial_capital = random.choice(initial_capitals)
        monthly_income = random.choice(monthly_incomes)
        target_income = random.choice(target_incomes)
        pension = target_income * random.uniform(0.3, 0.6)
        
        # Cr√©er le profil adulte
        adult = AdultProfile(
            first_name=f"Profile_{i+1}",
            current_age=float(age),
            retirement_age=float(retirement_age),
            life_expectancy=float(life_expectancy),
        )
        
        # Phases d'√©pargne
        savings_phases = [
            SavingsPhase(
                label="Capitalisation",
                from_age=float(age),
                to_age=float(retirement_age),
                monthly_contribution=monthly_income * random.uniform(0.1, 0.3),
            )
        ]
        
        # Comptes d'investissement
        allocation_actions = random.uniform(0.5, 0.9)
        investment_accounts = [
            InvestmentAccount(
                id=f"account_{i}",
                type=InvestmentAccountType.PEA if random.random() > 0.5 else InvestmentAccountType.ASSURANCE_VIE,
                label="Compte principal",
                current_amount=initial_capital,
                monthly_contribution=0.0,  # G√©r√© par les phases
                allocation_actions=allocation_actions * 100,
                allocation_obligations=(1.0 - allocation_actions) * 100,
            )
        ]
        
        # Profil de d√©penses
        spending_profile = [
            SpendingPhase(
                label="Retraite",
                from_age=float(retirement_age),
                to_age=float(life_expectancy),
                spending_ratio=random.uniform(0.7, 0.9),
            )
        ]
        
        # Cr√©er le profil d'optimisation
        profile = SavingsOptimizationInput(
            adults=[adult],
            savings_phases=savings_phases,
            investment_accounts=investment_accounts,
            market_assumptions=market_assumptions,
            spending_profile=spending_profile,
            target_monthly_income=target_income,
            state_pension_monthly_income=pension,
            target_final_capital=0.0,
            batch_size=50,
            max_iterations=10,
        )
        
        profiles.append(profile)
    
    return profiles


def train_general_model(
    num_profiles: int = 50,
    episodes_per_profile: int = 3000,
    model_name: str = "general",
    network_size: str = "solid",
) -> str:
    """
    Entra√Æne un mod√®le g√©n√©ralis√© sur plusieurs profils vari√©s.
    
    Args:
        num_profiles: Nombre de profils vari√©s √† utiliser
        episodes_per_profile: Nombre d'√©pisodes par profil
        model_name: Nom du mod√®le √† sauvegarder
        network_size: Taille du r√©seau ("standard", "solid", "robust", "enterprise")
    
    Returns:
        Chemin du mod√®le sauvegard√©
    """
    logger.info("=" * 70)
    logger.info("ENTRA√éNEMENT DU MOD√àLE RL G√âN√âRALIS√â")
    logger.info("=" * 70)
    logger.info(f"Profils: {num_profiles}")
    logger.info(f"√âpisodes par profil: {episodes_per_profile}")
    logger.info(f"Total d'√©pisodes: {num_profiles * episodes_per_profile:,}")
    logger.info(f"Taille du r√©seau: {network_size}")
    
    # G√©n√©rer les profils vari√©s
    logger.info(f"\nüìù G√©n√©ration de {num_profiles} profils vari√©s...")
    profiles = generate_varied_profiles(num_profiles)
    logger.info(f"‚úÖ {len(profiles)} profils g√©n√©r√©s")
    
    # Configuration du r√©seau selon la taille
    network_configs = {
        "standard": [128, 64, 32],
        "solid": [256, 128, 64, 32],
        "robust": [512, 256, 128, 64],
        "enterprise": [512, 512, 256, 128, 64],
    }
    
    # Configuration RL
    config = RLConfig(
        episodes=episodes_per_profile,
        hidden_layers=network_configs.get(network_size, network_configs["solid"]),
        mc_iterations_training=50,
        use_parallel=True,
    )
    
    # Optimiser pour M4 Pro
    config = optimize_rl_config_for_m4_pro(config)
    
    logger.info("=" * 70)
    logger.info(f"‚öôÔ∏è  CONFIGURATION DE L'ENTRA√éNEMENT")
    logger.info(f"   ‚Ä¢ Workers: {config.num_workers} (multiprocessing r√©el)")
    logger.info(f"   ‚Ä¢ Device: {config.device}")
    logger.info(f"   ‚Ä¢ R√©seau: {config.hidden_layers}")
    logger.info(f"   ‚Ä¢ Parall√©lisation: {'‚úÖ ACTIV√âE' if config.use_parallel and config.num_workers > 1 else '‚ùå D√âSACTIV√âE'}")
    logger.info("=" * 70)
    
    # Chemin de sauvegarde du mod√®le g√©n√©ral
    models_dir = Path(__file__).parent / "app" / "models" / "rl"
    models_dir.mkdir(parents=True, exist_ok=True)
    model_path = models_dir / f"rl_model_{model_name}_{network_size}.zip"
    
    logger.info(f"\nüéØ D√©but de l'entra√Ænement...")
    logger.info(f"   Mod√®le sera sauvegard√©: {model_path}")
    
    start_time = time.time()
    total_episodes = 0
    previous_model_path = None
    
    # Entra√Æner sur chaque profil
    for profile_idx, profile in enumerate(profiles):
        logger.info(f"\n{'='*70}")
        logger.info(f"üìä Profil {profile_idx + 1}/{len(profiles)}")
        logger.info(f"   √Çge: {profile.adults[0].current_age} ‚Üí {profile.adults[0].retirement_age} ans")
        logger.info(f"   Capital initial: {sum(acc.current_amount for acc in profile.investment_accounts):,.0f}‚Ç¨")
        
        # Cr√©er un nouveau trainer avec l'environnement vectoris√© pour ce profil
        # Le trainer g√©rera automatiquement la vectorisation avec les workers configur√©s
        trainer = RLTrainer(
            optimization_input=profile,
            config=config,
            model_path=None,  # Pas de sauvegarde automatique pendant l'entra√Ænement
        )
        
        # Charger le mod√®le pr√©c√©dent si on n'est pas au premier profil
        if previous_model_path and os.path.exists(previous_model_path):
            try:
                trainer.agent.load(previous_model_path)
                logger.info(f"   üîÑ Mod√®le pr√©c√©dent charg√© pour continuit√© d'entra√Ænement")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è  Impossible de charger le mod√®le pr√©c√©dent: {e}")
        
        # Entra√Æner sur ce profil (sans sauvegarde automatique)
        profile_start = time.time()
        stats = trainer.train(episodes=episodes_per_profile)
        profile_time = time.time() - profile_start
        total_episodes += episodes_per_profile
        
        logger.info(
            f"   ‚úÖ Termin√© en {profile_time/60:.1f} min - "
            f"R√©compense moyenne: {stats.get('mean_reward', 0):.3f}"
        )
        
        # Sauvegarder p√©riodiquement dans un fichier temporaire pour la continuit√©
        temp_model_path = str(model_path.parent / f"rl_model_{model_name}_{network_size}_temp.zip")
        trainer.agent.save(temp_model_path)
        previous_model_path = temp_model_path
        
        # Sauvegarder le checkpoint final p√©riodiquement
        if (profile_idx + 1) % 10 == 0 or profile_idx == len(profiles) - 1:
            trainer.agent.save(str(model_path))
            previous_model_path = str(model_path)
            logger.info(f"   üíæ Checkpoint sauvegard√© ({profile_idx + 1}/{len(profiles)} profils)")
        
        # Fermer proprement l'environnement vectoris√©
        if hasattr(trainer.env, 'close'):
            trainer.env.close()
    
    # Sauvegarder le mod√®le final
    if previous_model_path and os.path.exists(previous_model_path):
        import shutil
        shutil.copy(previous_model_path, str(model_path))
        logger.info(f"   üíæ Mod√®le final sauvegard√©")
    
    total_time = time.time() - start_time
    
    logger.info(f"\n{'='*70}")
    logger.info("‚úÖ ENTRA√éNEMENT TERMIN√â")
    logger.info(f"{'='*70}")
    logger.info(f"‚è±Ô∏è  Temps total: {total_time/60:.1f} minutes ({total_time/3600:.2f} heures)")
    logger.info(f"üìä Total d'√©pisodes: {total_episodes:,}")
    logger.info(f"üìÅ Mod√®le sauvegard√©: {model_path}")
    if model_path.exists():
        logger.info(f"üíæ Taille du mod√®le: {model_path.stat().st_size / (1024*1024):.2f} MB")
    
    return str(model_path)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Entra√Æner un mod√®le RL g√©n√©ralis√©")
    parser.add_argument(
        "--profiles",
        type=int,
        default=50,
        help="Nombre de profils vari√©s (d√©faut: 50)"
    )
    parser.add_argument(
        "--episodes",
        type=int,
        default=3000,
        help="Nombre d'√©pisodes par profil (d√©faut: 3000, total = profiles √ó episodes)"
    )
    parser.add_argument(
        "--network",
        type=str,
        default="solid",
        choices=["standard", "solid", "robust", "enterprise"],
        help="Taille du r√©seau (d√©faut: solid)"
    )
    parser.add_argument(
        "--name",
        type=str,
        default="general",
        help="Nom du mod√®le (d√©faut: general)"
    )
    parser.add_argument(
        "--yes",
        action="store_true",
        help="Ne pas demander de confirmation"
    )
    
    args = parser.parse_args()
    
    print("\nüöÄ Configuration:")
    print(f"   Profils: {args.profiles}")
    print(f"   √âpisodes par profil: {args.episodes}")
    print(f"   Total: {args.profiles * args.episodes:,} √©pisodes")
    print(f"   R√©seau: {args.network}")
    
    # Estimation du temps
    # Base: ~5 √©pisodes/seconde avec 12 workers
    estimated_time_min = (args.profiles * args.episodes) / (5 * 12) / 60
    print(f"   Temps estim√©: ~{estimated_time_min:.0f}-{estimated_time_min*1.5:.0f} minutes")
    
    if not args.yes:
        response = input("\n‚ñ∂Ô∏è  Continuer? (o/n): ")
        if response.lower() != 'o':
            print("Annul√©.")
            sys.exit(0)
    
    model_path = train_general_model(
        num_profiles=args.profiles,
        episodes_per_profile=args.episodes,
        model_name=args.name,
        network_size=args.network,
    )
    
    print(f"\nüéâ Mod√®le g√©n√©ral sauvegard√©: {model_path}")
    print("\nüí° Tous les utilisateurs peuvent maintenant utiliser ce mod√®le!")

